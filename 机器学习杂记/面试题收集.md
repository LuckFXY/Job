# 1. logistic

## 最小二乘法  

就是向量w对 $$E_w = (y-wx)^T(y-wx)$$ 求导，x满秩的时候有最优解

## 满秩

1. A的行列式不等于0
2. A的秩等于n，即A为满秩矩阵
3. A的行（列）向量组线性无关
4. 齐次方程组Ax=0只有零解
5. 对于任意b属于向量空间$R^n$，Ax=b总有唯一解
6. A与单位矩阵等价
7. A可表示成若干个初等矩阵的乘积
8. A的列向量可以作为n维向量空间$R^n$的一组基
9. Rn中任意一个向量都可以由A的列向量线性表出
10. A的特征值全不为0
11. AT·A是正定矩阵(其中T为上标，表示A的转置)
12. A是非奇异的

## 矩阵的秩

1. 将矩阵用高斯消元法化简后，非零行的行数叫做行秩，非零列的列数叫做列秩。 

2. 若A中至少有一个r阶子式不等于零，且在r< min(m,n)时，A中所有的r+1阶子式全为零，则A的秩为r。 

3. 「秩」是图像经过矩阵变换之后的空间维度。 

   1.  旋转矩阵的秩为2， 旋转后的空间维度也是2

   $$
   \left\{
   \begin{matrix}
   cos(θ) & −sin(θ)\\
   sin(θ) & cos(θ)
   \end{matrix}
   \right\}
   $$

4.  

## 线性方程式有解

AX = B 非奇异

增广矩阵 -> 阶梯型矩阵

* R(A)  = R(A;B)
  * R(A) < N 无数个解
  * R(A) = N 唯一解
* R(A) < R(A;B)
  * 无解

# 过拟合

## 合适的损失函数



1. svm， logistic， DT，KNN 推导加简单实现

2. SVM 和 对偶问题 松弛, 对偶问题更高效

3. L1 和 L2 正则

4. BP推导

5. 转置卷积

6. **互信息** 新词的左右比较丰富，有的老词的左右也比较丰富。还要区分出新词和老词。 

7. L1正则为什么可以把系数压缩成0，坐标下降法的具体实现细节 

8. spark 原理

   作者：Bloo_m
   链接：https://www.jianshu.com/p/eb2bc8d8ebc0
   來源：简书
   简书著作权归作者所有，任何形式的转载都请联系作者获得授权并注明出处。

   

   Spark是基于内存计算的大数据并行计算框架 

   （1）中间结果输出
   基于MapReduce的计算引擎通常会将中间结果输出到磁盘上，进行存储和容错。出于
   任务管道承接的考虑，当一些查询翻译到MapReduce任务时，往往会产生多个Stage，而
   这些串联的Stage又依赖于底层文件系统（如HDFS）来存储每一个Stage的输出结果。
   Spark将执行模型抽象为通用的有向无环图执行计划（DAG），这可以将多Stage的任
   务串联或者并行执行，而无须将Stage中间结果输出到HDFS中。类似的引擎包括Dr yad、
   Tez。
   （2）数据格式和内存布局
   Spark抽象出分布式内存存储结构弹性分布式数据集RDD,进行数据的存储。RDD能支持粗粒度写操作,但对于读取操作,RDD可以精确到每条记录,这使得RDD可以用来作为分布式索引
   Spark的特性是能够控制数据在不同节点上的分区,用户可以自定义分区策略,如Hash分区等.Shark和Spark SQL在Spark的基础上实现了列存储和列存储压缩
   （3）执行策略
   Spark任务在shuffle中不是所有情景都需要排序,所以支持基于Hash的分布式聚合,调度中采用更为通用的任务执行计划图(DAG),每一轮次的输出结果在内存缓存
   （4）任务调度的开销
   传统的MapReduce系统,是为了运行长达数小时的批量作业而设计的,在某些极端的情况下,提交一个任务的延迟非常高
   Spark采用了事件驱动的类库AKKA来启动任务,通过线程池复用线程来避免进程或线程启动和切换开销



作者：Bloo_m
链接：https://www.jianshu.com/p/eb2bc8d8ebc0
來源：简书
简书著作权归作者所有，任何形式的转载都请联系作者获得授权并注明出处。

·ClusterManager：在Standalone模式中即为Master（主节点），控制整个集群，监控Worker。在YARN模式中为资源管理器。

Worker：从节点，负责控制计算节点，启动Executor或Driver。在YARN模式中为NodeManager，负责计算节点的控制

·Driver：运行Application的main（）函数并创建SparkContext。

·Executor：执行器，在worker node上执行任务的组件、用于启动线程池运行任务。每个Application拥有独立的一组Executors。

·SparkContext：整个应用的上下文，控制应用的生命周期

·RDD：Spark的基本计算单元，一组RDD可形成执行的有向无环图RDD Graph

·DAG Scheduler：根据作业（Job）构建基于Stage的DAG，并提交Stage给
TaskScheduler。

·TaskScheduler：将任务（Task）分发给Executor执行。

·TaskScheduler：将任务（Task）分发给Executor执行。

·SparkEnv：线程级别的上下文，存储运行时的重要组件的引用。



1. spark Executor memory 给16G  executor core 给2个。问每个core分配多少内存 
2. Spark是多线程模式，怎么退化为多进程模式。
3. 在每个executor core设置为1，即每个executor是单线程的。
4. 线程安全
5. 一个表可以没有主键，可以有索引 
6. DT 过拟合 剪枝，前剪枝和后剪枝。说了REP剪枝。C4.5是悲观剪枝 
7. 距离度量！！
8. 优化器， 拟牛顿法
9. GDBT
10. LSTM



连续子数组的最大乘积

```python
def maxSubProduct(arr):
    n = len(arr)
    if(n == 0):
        return 0
    max_arr = [0] * n;
    min_arr = [0] * n;
    product = max_arr[0] = min_arr[0] = arr[0]
    for i in range(1,n):
        max_arr[i] = max(a[i], a[i]*max_arr[i-1], a[i]*min_arr[i-1])
        min_arr[i] = min(a[i], a[i]*max_arr[i-1], a[i]*min_arr[i-1])
        product = max(product, max_arr[i])
    return product
```

