{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import findspark\n",
    "# findspark.init(\"/usr/local/spark\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import subprocess\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pi is roughtly 3.138912\n"
     ]
    }
   ],
   "source": [
    "sc = SparkContext()\n",
    "def inside(p):\n",
    "    x,y = random.random(), random.random()\n",
    "    return x*x + y*y < 1\n",
    "NUM_SAMPLES = 1000000\n",
    "count = sc.parallelize(range(0, NUM_SAMPLES)).filter(inside).count()\n",
    "print(\"Pi is roughtly %f\"%(4.0 * count / NUM_SAMPLES))\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start a spark task\n",
    "sc = SparkContext()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "要加载本地文件，必须采用 \"file:///” 开头的这种格式。执行上上面这条命令以后，并不会马上显示结果，因为，Spark采用惰性机制，只有遇到“行动”类型的操作，才会从头到尾执行所有操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/media/fxy/study/GitHub/Job/Spark/code/test_spark/word.txt\n",
      "/media/fxy/study/GitHub/Job/Spark/code/test_spark/output\n"
     ]
    }
   ],
   "source": [
    "filepath = \"word.txt\"\n",
    "filepath = os.path.realpath(filepath)\n",
    "print(filepath)\n",
    "output = \"output\"\n",
    "output = os.path.realpath(output)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file:///media/fxy/study/GitHub/Job/Spark/code/test_spark/word.txt\n"
     ]
    }
   ],
   "source": [
    "#load txt file from local disk\n",
    "local_disk = r\"file://\"\n",
    "path = local_disk + filepath\n",
    "print(path)\n",
    "textFile = sc.textFile(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "first()是一个“行动”（Action）类型的操作，会启动真正的计算过程，从文件中加载数据到变量textFile中，并取出第一行文本。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Chinese researchers have developed interfacially polymerized porous polymer particles for low- abundance glycopeptide separation. These polymer particles – with hydrophilic-hydrophobic heterostructured nanopores – can separate low-abundance glycopeptides from complex biological samples with high-abundance background molecules efficiently. '"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textFile.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "saveAsTextFile()是一个“行动”（Action）类型的操作，所以，马上会执行真正的计算过程，从word.txt中加载数据到变量textFile中，然后，又把textFile中的数据写回到本地文件目录"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "folder exists!!!\n"
     ]
    }
   ],
   "source": [
    "if os.path.exists(output) == False:\n",
    "    out_path = local_disk + output\n",
    "    textFile.saveAsTextFile(out_path)\n",
    "else:\n",
    "    print(\"folder exists!!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.part-00000.crc',\n",
       " '.part-00001.crc',\n",
       " '._SUCCESS.crc',\n",
       " 'part-00000',\n",
       " 'part-00001',\n",
       " '_SUCCESS']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 加载HDFS文件 (只有hadoop用户才有权限)\n",
    "主程序 hdfs_head = r\"usr/local/hadoop/bin/hdfs\n",
    "1. 创建一个hdfs 文件 /user/hadoop 这个目录是在HDFS文件系统中，不在本地文件系统中。\n",
    "2. 讲本地文件导入hdfs文件系统中\n",
    "3. 用pyspark 加载 hdfs中的文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_cmd(cmd, print_cmd = False, input_pwd=False):\n",
    "    if(print_cmd):\n",
    "        print(\"cmd : \",cmd)\n",
    "    p = subprocess.Popen(cmd, shell=True, \n",
    "                         stdin = subprocess.PIPE, stderr=subprocess.PIPE, \n",
    "                         stdout=subprocess.PIPE, universal_newlines=True)\n",
    "        \n",
    "    if(input_pwd):\n",
    "        time.sleep(1)\n",
    "        p.stdin.write(\"123\\n\")\n",
    "    p.stdin.close()\n",
    "    \n",
    "    stderr = p.stderr\n",
    "    if(stderr):\n",
    "        print(stderr.readlines())\n",
    "    if(not input_pwd):\n",
    "        out = p.stdout.readlines()\n",
    "        for row in out:\n",
    "            print(row.strip())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hdfs_ops(cmd=r\"ls /\", print_cmd=True):\n",
    "    hdfs_head = r\"/usr/local/hadoop/bin/hdfs dfs -\"\n",
    "    cmd = hdfs_head + cmd\n",
    "\n",
    "    do_cmd(cmd, print_cmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cmd :  /usr/local/hadoop/bin/hdfs dfs -mkdir -p /user/spark\n",
      "[]\n",
      "cmd :  /usr/local/hadoop/bin/hdfs dfs -ls -R /\n",
      "[]\n",
      "drwxr-xr-x   - hadoop supergroup          0 2018-08-16 20:26 /user\n",
      "drwxr-xr-x   - hadoop supergroup          0 2018-08-16 20:26 /user/spark\n"
     ]
    }
   ],
   "source": [
    "# create folder spark\n",
    "#hdfs_ops(\"rm -r /user\")\n",
    "hdfs_ops(\"mkdir -p /user/spark\")\n",
    "hdfs_ops(\"ls -R /\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cmd :  /usr/local/hadoop/bin/hdfs dfs -rm /user/spark/word.txt\n"
     ]
    }
   ],
   "source": [
    "hdfs_ops(\"rm /user/spark/word.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "def copy_to_hadoop(path, new_path = None, print_cmd = True):\n",
    "    \n",
    "    if(new_path is None):\n",
    "        new_path = \"/home/hadoop/mydata\"\n",
    "        \n",
    "    print(\"\\n-------copy from %s to %s----------\"%(path, new_path))\n",
    "    \n",
    "    filename = os.path.split(path)[-1]\n",
    "    new_path = os.path.join(new_path, filename)\n",
    "    if(os.path.exists(new_path) == False):\n",
    "        cmd1 = \"sudo -S cp %s %s\\n\"%(path, new_path)\n",
    "        do_cmd(cmd1,print_cmd, input_pwd=True)\n",
    "    cmd2 = \"sudo -S chown hadoop:hadoop %s\"%new_path\n",
    "    do_cmd(cmd2,print_cmd, input_pwd=True)\n",
    "    cmd3 = \"sudo -S chmod 664 %s\"%new_path\n",
    "    do_cmd(cmd3, print_cmd, input_pwd=True)\n",
    "    cmd4 = \"ls -l %s\"%new_path\n",
    "    do_cmd(cmd4, print_cmd)\n",
    "    print(\"------------copy finished-----------\\n\")\n",
    "    return new_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/media/fxy/study/GitHub/Job/Spark/code/test_spark/word.txt\n",
      "True\n",
      "\n",
      "-------copy from /media/fxy/study/GitHub/Job/Spark/code/test_spark/word.txt to None----------\n",
      "cmd :  sudo -S chown hadoop:hadoop /home/hadoop/mydata/word.txt\n",
      "['[sudo] fxy 的密码： ']\n",
      "cmd :  sudo -S chmod 664 /home/hadoop/mydata/word.txt\n",
      "['[sudo] fxy 的密码： ']\n",
      "cmd :  ls -l /home/hadoop/mydata/word.txt\n",
      "[]\n",
      "-rw-rw-r-- 1 hadoop hadoop 2503 8月  16 22:09 /home/hadoop/mydata/word.txt\n",
      "------------copy finished-----------\n",
      "\n",
      "cmd :  /usr/local/hadoop/bin/hdfs dfs -put /home/hadoop/mydata/word.txt /user/spark/\n",
      "[\"put: `/user/spark/word.txt': File exists\\n\"]\n",
      "cmd :  /usr/local/hadoop/bin/hdfs dfs -ls /user/spark\n",
      "[]\n",
      "Found 1 items\n",
      "-rw-r--r--   1 hadoop supergroup       2503 2018-08-16 22:31 /user/spark/word.txt\n"
     ]
    }
   ],
   "source": [
    "print(filepath)\n",
    "print(os.path.exists(filepath))\n",
    "#header = r\"file://\"\n",
    "#filepath2 = header + filepath\n",
    "new_path = copy_to_hadoop(filepath)\n",
    "hdfs_ops(\"put %s /user/spark/\"%new_path)\n",
    "hdfs_ops(\"ls /user/spark\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sc.textFile(“hdfs://localhost:9000/user/hadoop/word.txt”)中，“hdfs://localhost:9000/”是前面介绍Hadoop安装内容时确定下来的端口地址9000。实际上，也可以省略不写，如下三条语句都是等价的：\n",
    "1. > textFile = sc.textFile(\"hdfs://localhost:9000/user/hadoop/word.txt\")\n",
    "2. > textFile = sc.textFile(\"/user/hadoop/word.txt\")\n",
    "3. > textFile = sc.textFile(\"word.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Chinese researchers have developed interfacially polymerized porous polymer particles for low- abundance glycopeptide separation. These polymer particles – with hydrophilic-hydrophobic heterostructured nanopores – can separate low-abundance glycopeptides from complex biological samples with high-abundance background molecules efficiently. '"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textFile.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面，我们再把textFile的内容写回到HDFS文件系统中（写到hadoop用户目录下）："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['.part-00000.crc', '.part-00001.crc', '._SUCCESS.crc', 'part-00000', 'part-00001', '_SUCCESS']\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "output = \"writeback\"\n",
    "if(os.path.exists(output)):\n",
    "    shutil.rmtree(output)\n",
    "textFile.saveAsTextFile(output)\n",
    "print(os.listdir(output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 有了前面的铺垫性介绍，下面我们就可以开始第一个Spark应用程序：WordCount。\n",
    "\n",
    "\n",
    "### 1. textFile.flatMap(labmda line : line.split(” “))\n",
    "会遍历textFile中的每行文本内容，当遍历到其中一行文本内容时，会把文本内容赋值给变量line，并执行Lamda表达式line : line.split(” “).多行文本，采用空格作为分隔符进行单词切分，就得到多个单词集合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = textFile.flatMap(lambda line: line.split(\" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Chinese\\n', 'researchers\\n', 'have\\n', 'developed\\n', 'interfacially\\n', 'polymerized\\n', 'porous\\n', 'polymer\\n', 'particles\\n', 'for\\n', 'low-\\n', 'abundance\\n', 'glycopeptide\\n', 'separation.\\n', 'These\\n', 'polymer\\n', 'particles\\n', '–\\n', 'with\\n', 'hydrophilic-hydrophobic\\n', 'heterostructured\\n', 'nanopores\\n', '–\\n', 'can\\n', 'separate\\n', 'low-abundance\\n', 'glycopeptides\\n', 'from\\n', 'complex\\n', 'biological\\n', 'samples\\n', 'with\\n', 'high-abundance\\n', 'background\\n', 'molecules\\n', 'efficiently.\\n', '\\n', '\\n', 'Results\\n', 'were\\n', 'published\\n', 'in\\n', 'the\\n', 'journal\\n', 'Advanced\\n', 'Materials\\n', 'in\\n', 'an\\n', 'article\\n', 'entitled\\n', '\"Interfacially\\n', 'Polymerized\\n', 'Particles\\n', 'with\\n', 'Heterostructured\\n', 'Nanopores\\n', 'for\\n', 'Glycopeptide\\n', 'Separation.\"\\n', '\\n', '\\n', 'Qualitative/quantitative\\n', 'analyses\\n', 'of\\n', 'low-abundance\\n', 'biomolecules\\n', 'from\\n', 'complex\\n', 'biofluids\\n', 'are\\n', 'critical\\n', 'in\\n', 'clinical\\n', 'diagnosis\\n', 'and\\n', 'prognosis.\\n', 'For\\n', 'example,\\n', 'glycosylated\\n', 'Aβ\\n', 'peptide\\n', 'can\\n', 'help\\n', 'identify\\n', \"Alzheimer's\\n\", 'disease\\n', 'and\\n', 'circulating\\n', 'tumor\\n', 'DNA\\n', 'can\\n', 'help\\n', 'identify\\n', 'cancer.\\n', '\\n', '\\n', 'Porous\\n', 'polymer\\n', 'materials\\n', 'have\\n', 'been\\n', 'extensively\\n', 'used\\n', 'for\\n', 'separation.\\n', 'However,\\n', 'most\\n', 'existing\\n', 'porous\\n', 'polymer\\n', 'materials\\n', 'have\\n', 'homogeneous\\n', 'compositions\\n', 'or\\n', 'pores.\\n', 'As\\n', 'a\\n', 'result,\\n', 'efficiently\\n', 'and\\n', 'specifically\\n', 'separating\\n', 'subsets\\n', 'of\\n', 'low-abundance\\n', 'biomolecules\\n', 'from\\n', 'complex\\n', 'samples\\n', '(such\\n', 'as\\n', 'serum\\n', 'and\\n', 'plasma)\\n', 'is\\n', 'a\\n', 'great\\n', 'challenge.\\n', 'Although\\n', 'recent\\n', 'molecule-level\\n', 'surface\\n', 'modification\\n', 'efforts\\n', 'for\\n', 'these\\n', 'homogeneous\\n', 'porous\\n', 'polymer\\n', 'materials\\n', 'have\\n', 'demonstrated\\n', 'enhanced\\n', 'separation\\n', 'specificity,\\n', 'unspecific\\n', 'adsorption\\n', 'of\\n', 'high-abundance\\n', 'background\\n', 'molecules\\n', 'still\\n', 'exists.\\n', '\\n']\n"
     ]
    }
   ],
   "source": [
    "output = \"words\"\n",
    "output = os.path.realpath(output)\n",
    "if(os.path.exists(output)):\n",
    "    shutil.rmtree(output)\n",
    "    \n",
    "words.saveAsTextFile(output)\n",
    "\n",
    "tmp = os.path.join(output,\"part-00000\")\n",
    "with open(tmp,\"r\") as f:\n",
    "    print(f.readlines())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. word : (word, 1)\n",
    "这个Lamda表达式的含义是，word作为函数的输入参数，然后，执行函数处理逻辑，这里会执行(word, 1)，也就是针对输入的word，构建得到一个tuple，形式为(word,1)，key是word，value是1（表示该单词出现1次）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_tuples = words.map(lambda word:(word,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"('Chinese', 1)\\n\", \"('researchers', 1)\\n\", \"('have', 1)\\n\", \"('developed', 1)\\n\", \"('interfacially', 1)\\n\", \"('polymerized', 1)\\n\", \"('porous', 1)\\n\", \"('polymer', 1)\\n\", \"('particles', 1)\\n\", \"('for', 1)\\n\", \"('low-', 1)\\n\", \"('abundance', 1)\\n\", \"('glycopeptide', 1)\\n\", \"('separation.', 1)\\n\", \"('These', 1)\\n\", \"('polymer', 1)\\n\", \"('particles', 1)\\n\", \"('–', 1)\\n\", \"('with', 1)\\n\", \"('hydrophilic-hydrophobic', 1)\\n\", \"('heterostructured', 1)\\n\", \"('nanopores', 1)\\n\", \"('–', 1)\\n\", \"('can', 1)\\n\", \"('separate', 1)\\n\", \"('low-abundance', 1)\\n\", \"('glycopeptides', 1)\\n\", \"('from', 1)\\n\", \"('complex', 1)\\n\", \"('biological', 1)\\n\", \"('samples', 1)\\n\", \"('with', 1)\\n\", \"('high-abundance', 1)\\n\", \"('background', 1)\\n\", \"('molecules', 1)\\n\", \"('efficiently.', 1)\\n\", \"('', 1)\\n\", \"('', 1)\\n\", \"('Results', 1)\\n\", \"('were', 1)\\n\", \"('published', 1)\\n\", \"('in', 1)\\n\", \"('the', 1)\\n\", \"('journal', 1)\\n\", \"('Advanced', 1)\\n\", \"('Materials', 1)\\n\", \"('in', 1)\\n\", \"('an', 1)\\n\", \"('article', 1)\\n\", \"('entitled', 1)\\n\", '(\\'\"Interfacially\\', 1)\\n', \"('Polymerized', 1)\\n\", \"('Particles', 1)\\n\", \"('with', 1)\\n\", \"('Heterostructured', 1)\\n\", \"('Nanopores', 1)\\n\", \"('for', 1)\\n\", \"('Glycopeptide', 1)\\n\", '(\\'Separation.\"\\', 1)\\n', \"('', 1)\\n\", \"('', 1)\\n\", \"('Qualitative/quantitative', 1)\\n\", \"('analyses', 1)\\n\", \"('of', 1)\\n\", \"('low-abundance', 1)\\n\", \"('biomolecules', 1)\\n\", \"('from', 1)\\n\", \"('complex', 1)\\n\", \"('biofluids', 1)\\n\", \"('are', 1)\\n\", \"('critical', 1)\\n\", \"('in', 1)\\n\", \"('clinical', 1)\\n\", \"('diagnosis', 1)\\n\", \"('and', 1)\\n\", \"('prognosis.', 1)\\n\", \"('For', 1)\\n\", \"('example,', 1)\\n\", \"('glycosylated', 1)\\n\", \"('Aβ', 1)\\n\", \"('peptide', 1)\\n\", \"('can', 1)\\n\", \"('help', 1)\\n\", \"('identify', 1)\\n\", '(\"Alzheimer\\'s\", 1)\\n', \"('disease', 1)\\n\", \"('and', 1)\\n\", \"('circulating', 1)\\n\", \"('tumor', 1)\\n\", \"('DNA', 1)\\n\", \"('can', 1)\\n\", \"('help', 1)\\n\", \"('identify', 1)\\n\", \"('cancer.', 1)\\n\", \"('', 1)\\n\", \"('', 1)\\n\", \"('Porous', 1)\\n\", \"('polymer', 1)\\n\", \"('materials', 1)\\n\", \"('have', 1)\\n\", \"('been', 1)\\n\", \"('extensively', 1)\\n\", \"('used', 1)\\n\", \"('for', 1)\\n\", \"('separation.', 1)\\n\", \"('However,', 1)\\n\", \"('most', 1)\\n\", \"('existing', 1)\\n\", \"('porous', 1)\\n\", \"('polymer', 1)\\n\", \"('materials', 1)\\n\", \"('have', 1)\\n\", \"('homogeneous', 1)\\n\", \"('compositions', 1)\\n\", \"('or', 1)\\n\", \"('pores.', 1)\\n\", \"('As', 1)\\n\", \"('a', 1)\\n\", \"('result,', 1)\\n\", \"('efficiently', 1)\\n\", \"('and', 1)\\n\", \"('specifically', 1)\\n\", \"('separating', 1)\\n\", \"('subsets', 1)\\n\", \"('of', 1)\\n\", \"('low-abundance', 1)\\n\", \"('biomolecules', 1)\\n\", \"('from', 1)\\n\", \"('complex', 1)\\n\", \"('samples', 1)\\n\", \"('(such', 1)\\n\", \"('as', 1)\\n\", \"('serum', 1)\\n\", \"('and', 1)\\n\", \"('plasma)', 1)\\n\", \"('is', 1)\\n\", \"('a', 1)\\n\", \"('great', 1)\\n\", \"('challenge.', 1)\\n\", \"('Although', 1)\\n\", \"('recent', 1)\\n\", \"('molecule-level', 1)\\n\", \"('surface', 1)\\n\", \"('modification', 1)\\n\", \"('efforts', 1)\\n\", \"('for', 1)\\n\", \"('these', 1)\\n\", \"('homogeneous', 1)\\n\", \"('porous', 1)\\n\", \"('polymer', 1)\\n\", \"('materials', 1)\\n\", \"('have', 1)\\n\", \"('demonstrated', 1)\\n\", \"('enhanced', 1)\\n\", \"('separation', 1)\\n\", \"('specificity,', 1)\\n\", \"('unspecific', 1)\\n\", \"('adsorption', 1)\\n\", \"('of', 1)\\n\", \"('high-abundance', 1)\\n\", \"('background', 1)\\n\", \"('molecules', 1)\\n\", \"('still', 1)\\n\", \"('exists.', 1)\\n\", \"('', 1)\\n\"]\n"
     ]
    }
   ],
   "source": [
    "output = \"word_tuples\"\n",
    "output = os.path.realpath(output)\n",
    "if(os.path.exists(output)):\n",
    "    shutil.rmtree(output)\n",
    "    \n",
    "word_tuples.saveAsTextFile(output)\n",
    "\n",
    "tmp = os.path.join(output,\"part-00000\")\n",
    "with open(tmp,\"r\") as f:\n",
    "    print(f.readlines())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### reduceByKey(labmda a, b : a + b)操作，\n",
    "得到一个RDD，这个RDD的每个元素是(key,value)形式的tuple。最后，针对这个RDD，执行reduceByKey(labmda a, b : a + b)操作，这个操作会把所有RDD元素按照key进行分组，然后使用给定的函数（这里就是Lamda表达式：a, b : a + b），对具有相同的key的多个value进行reduce操作，返回reduce后的(key,value)，比如(“hadoop”,1)和(“hadoop”,1)，具有相同的key，进行reduce以后就得到(“hadoop”,2)，"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordCount = word_tuples.reduceByKey(lambda a, b : a+b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            ('researchers', 2)                    ('have', 4)               ('developed', 2) \n",
      "          ('interfacially', 1)             ('polymerized', 1)               ('particles', 4) \n",
      "              ('abundance', 3)             ('separation.', 2)                   ('These', 1) \n",
      "          ('low-abundance', 4)           ('glycopeptides', 4)               ('molecules', 3) \n",
      "           ('efficiently.', 1)                       ('', 16)               ('published', 1) \n",
      "                     ('in', 5)                      ('an', 2)                ('entitled', 1) \n",
      "            ('Polymerized', 1)               ('Nanopores', 1)            ('Glycopeptide', 1) \n",
      "           ('Separation.\"', 1)                ('analyses', 1)                     ('of', 12) \n",
      "           ('biomolecules', 3)               ('biofluids', 1)                     ('are', 2) \n",
      "              ('diagnosis', 1)                ('example,', 1)                 ('peptide', 1) \n",
      "                   ('help', 2)                ('identify', 2)             (\"Alzheimer's\", 2) \n",
      "            ('circulating', 1)                   ('tumor', 1)                 ('cancer.', 2) \n",
      "                 ('Porous', 1)                    ('used', 1)                ('However,', 2) \n",
      "            ('homogeneous', 2)            ('compositions', 1)                  ('pores.', 1) \n",
      "             ('separating', 1)                 ('subsets', 1)                   ('(such', 2) \n",
      "                     ('as', 3)                 ('plasma)', 1)                      ('is', 3) \n",
      "           ('modification', 1)                 ('efforts', 1)                   ('these', 1) \n",
      "           ('demonstrated', 1)                ('enhanced', 1)              ('separation', 4) \n",
      "           ('specificity,', 1)              ('unspecific', 1)              ('Endogenous', 1) \n",
      "            ('significant', 1)              ('biomarkers', 1)                  ('always', 1) \n",
      "                    ('low', 1)                      ('10', 1)                   ('pg/mL', 1) \n",
      "             ('biofluids)', 1)                    ('high', 1)                ('proteins', 2) \n",
      "             ('biofluids.', 1)             ('Researchers', 1)               ('Technical', 1) \n",
      "              ('Institute', 1)                ('Sciences', 1)                ('emulsion', 2) \n",
      "            ('interfacial', 2)                ('approach', 1)                   ('Janus', 1) \n",
      "             ('actuators.', 1)                   ('basis', 1)                ('studies,', 1) \n",
      "                 ('series', 1)      ('non-glycopeptides.', 1)                     ('The', 1) \n",
      "               ('allowing', 1)                     ('via', 1) "
     ]
    }
   ],
   "source": [
    "output = \"wordCount\"\n",
    "output = os.path.realpath(output)\n",
    "if(os.path.exists(output)):\n",
    "    shutil.rmtree(output)\n",
    "    \n",
    "wordCount.saveAsTextFile(output)\n",
    "\n",
    "tmp = os.path.join(output,\"part-00000\")\n",
    "with open(tmp,\"r\") as f:\n",
    "    cnt = 1\n",
    "    for row in f.readlines():\n",
    "        print(\"%30s\"%row.strip(), end=' ')\n",
    "        if cnt == 3:\n",
    "            cnt = 0\n",
    "            print('')\n",
    "        cnt += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the output in the terminal, sad\n",
    "#wordCount.foreachPartition(myprint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
